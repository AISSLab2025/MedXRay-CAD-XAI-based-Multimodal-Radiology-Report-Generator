{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de91d230-cdcb-40fb-811f-494ece80c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, precision_score, roc_curve, f1_score, auc, recall_score, accuracy_score, classification_report, multilabel_confusion_matrix\n",
    "from transformers import ViTModel, ViTFeatureExtractor\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import json\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from skimage import exposure\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6bdeb-180e-4c37-8f5e-491ccee219c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90a82f1-2a5d-4027-a022-42dcffa663c8",
   "metadata": {},
   "source": [
    "## Implementation (CheXclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09265f27-45cc-4990-b6e8-a48a83fab9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"path_to_directory\"\n",
    "dataset = pd.read_csv(\"path_to_directory\")\n",
    "len(dataset)\n",
    "# # Get the first 100 records\n",
    "# dataset = dataset.head(100)\n",
    "\n",
    "# # Display the length of the subset to confirm\n",
    "# print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c2122-2229-450a-8ca0-1184791cd8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dirs = ['p10', 'p11', 'p12', 'p13', 'p14', 'p15', 'p16', 'p17', 'p18', 'p19']\n",
    "\n",
    "# Function to check if the path format is correct\n",
    "def check_path_format(row, image_column):\n",
    "    image_path = os.path.normpath(row[image_column])\n",
    "    \n",
    "    # Split the path components\n",
    "    parts = image_path.split(os.sep)  # os.sep is platform-specific separator\n",
    "    \n",
    "    if len(parts) < 4:\n",
    "        return False\n",
    "    \n",
    "    # Check the main directory is valid\n",
    "    main_dir = parts[0]\n",
    "    if main_dir not in main_dirs:\n",
    "        return False\n",
    "    \n",
    "    subject_id = 'p' + str(row['subject_id'])\n",
    "    if parts[1] != subject_id:\n",
    "        return False\n",
    "    \n",
    "    study_id = 's' + str(row['study_id'])\n",
    "    if parts[2] != study_id:\n",
    "        return False\n",
    "    \n",
    "    # Check if file exists at the expected location\n",
    "    full_path = os.path.join(BASE_PATH, *parts)\n",
    "    if not os.path.exists(full_path):\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "dataset['frontal_image_valid'] = dataset.apply(lambda row: check_path_format(row, 'frontal_image'), axis=1)\n",
    "\n",
    "# Filter out rows with mismatches\n",
    "invalid_rows = dataset[(~dataset['frontal_image_valid'])]\n",
    "\n",
    "# Save invalid rows to a CSV file for inspection\n",
    "# invalid_rows.to_csv('invalid_image_paths.csv', index=False)\n",
    "print(f\"Number of invalid rows: {len(invalid_rows)}\")\n",
    "# print(\"Invalid rows saved to 'invalid_image_paths.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbec046c-01d2-4c6f-9547-3879091cee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion', 'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e84b9c-ccd7-433a-a2c2-d6441b9c36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_data = train_test_split(dataset, test_size=0.2, random_state=42)  # 80% training\n",
    "valid_df, test_df = train_test_split(temp_data, test_size=0.5, random_state=42)  # 10% validation, 10% testing\n",
    "\n",
    "# Display the lengths of the datasets\n",
    "print(f'Number of samples in training set: {len(train_df)}')\n",
    "print(f'Number of samples in validation set: {len(valid_df)}')\n",
    "print(f'Number of samples in test set: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b91f9b9-56a7-41be-b3a8-c8486ce06524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_leakage(df1, df2, study_col):\n",
    "    df1_studies_unique = set(df1[study_col].unique().tolist())\n",
    "    df2_studies_unique = set(df2[study_col].unique().tolist())\n",
    "    \n",
    "    # Check for any common studies in both datasets\n",
    "    studies_in_both_groups = df1_studies_unique.intersection(df2_studies_unique)\n",
    "    leakage = len(studies_in_both_groups) >= 1\n",
    "    return leakage\n",
    "\n",
    "# Check for leakage using the 'study_id' column\n",
    "print(\"Leakage between train and test: {}\".format(check_for_leakage(train_df, test_df, 'study_id')))\n",
    "print(\"Leakage between valid and test: {}\".format(check_for_leakage(valid_df, test_df, 'study_id')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac9e67-8325-478d-a1ba-a9e23832818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_paths(df):\n",
    "    image_paths = {\n",
    "        \"frontal_images\": df['frontal_image'].dropna().tolist(),\n",
    "        \"lateral_images\": df['lateral_image'].dropna().tolist(),\n",
    "    }\n",
    "    return image_paths\n",
    "\n",
    "# Extract image paths for each dataset\n",
    "train_image_paths = extract_image_paths(train_df)\n",
    "valid_image_paths = extract_image_paths(valid_df)\n",
    "test_image_paths = extract_image_paths(test_df)\n",
    "\n",
    "# Save each split to a JSON file\n",
    "def save_to_json(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Save the image paths\n",
    "save_to_json(train_image_paths, 'train_image_paths.json')\n",
    "save_to_json(valid_image_paths, 'valid_image_paths.json')\n",
    "save_to_json(test_image_paths, 'test_image_paths.json')\n",
    "\n",
    "print(\"Image paths saved successfully in JSON format!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec413f8-cc25-493c-bdd3-26fc90e671e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_img = Image.open(\"path_to_directory\")\n",
    "img = np.asarray(pil_img).astype('uint8')\n",
    "print(img.max())\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f133410-9f6c-4371-84ba-8196371f5e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLAHETransform:\n",
    "    def __init__(self, clip_limit=0.10, tile_grid_size=(8, 8)):\n",
    "        self.clip_limit = clip_limit\n",
    "        self.tile_grid_size = tile_grid_size\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Convert PIL image to numpy array if necessary\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = np.array(img)\n",
    "        \n",
    "        # If the image is RGB (3 channels), convert it to LAB color space\n",
    "        if img.ndim == 3:\n",
    "            lab_img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "            l_channel, a_channel, b_channel = cv2.split(lab_img)\n",
    "\n",
    "            # Apply CLAHE only to the L (lightness) channel\n",
    "            l_channel = self.clahe.apply(l_channel)\n",
    "\n",
    "            # Merge back and convert to RGB\n",
    "            lab_img = cv2.merge((l_channel, a_channel, b_channel))\n",
    "            img = cv2.cvtColor(lab_img, cv2.COLOR_LAB2RGB)\n",
    "        else:\n",
    "            # If the image is grayscale, apply CLAHE directly\n",
    "            img = self.clahe.apply(img)\n",
    "\n",
    "        # Convert back to PIL Image before returning\n",
    "        return Image.fromarray(img.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b58d85-c852-4058-888f-2427e8370022",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MimicCXR_Dataset(Dataset):\n",
    "    def __init__(self, img_data, img_path, labels, transform=None):\n",
    "        self.img_data = img_data\n",
    "        self.img_path = img_path\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get frontal image path\n",
    "        frontal_img_name = self.img_data.iloc[index]['frontal_image']\n",
    "        frontal_img_name = os.path.join(self.img_path, str(frontal_img_name)) if pd.notna(frontal_img_name) else None\n",
    "        \n",
    "        # Get lateral image path\n",
    "        lateral_img_name = self.img_data.iloc[index]['lateral_image']  # Assuming you have this column in the CSV\n",
    "        lateral_img_name = os.path.join(self.img_path, str(lateral_img_name)) if pd.notna(lateral_img_name) else None\n",
    "        \n",
    "        # Check if the paths are valid\n",
    "        if frontal_img_name is None or not os.path.exists(frontal_img_name):\n",
    "            raise FileNotFoundError(f'Frontal image not found at path: {frontal_img_name}')\n",
    "        if lateral_img_name is None or not os.path.exists(lateral_img_name):\n",
    "            raise FileNotFoundError(f'Lateral image not found at path: {lateral_img_name}')\n",
    "        \n",
    "        # Open both images\n",
    "        frontal_image = Image.open(frontal_img_name).convert('RGB')  # Convert to 3-channel RGB\n",
    "        lateral_image = Image.open(lateral_img_name).convert('RGB')  # Convert to 3-channel RGB\n",
    "        \n",
    "        # Fetch label, handle NaNs before converting to tensor\n",
    "        label = self.img_data.iloc[index][self.labels].fillna(0).values  # Replace NaNs with 0\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            frontal_image = self.transform(frontal_image)\n",
    "            lateral_image = self.transform(lateral_image)\n",
    "        \n",
    "        # Return both images (frontal and lateral) and the label\n",
    "        return (frontal_image, lateral_image), label\n",
    "\n",
    "        \n",
    "def calculate_mean_std(self):\n",
    "    # Temporary transform to convert images to tensors\n",
    "    # Initialize variables to store sums\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    num_images = 0\n",
    "    \n",
    "    # Iterate over all images\n",
    "    for img_file in self.img_data['frontal_image'].to_list():\n",
    "        img_path = os.path.join(self.img_path, img_file)\n",
    "        pil_img = Image.open(img_path)\n",
    "        img = np.asarray(pil_img).astype('uint8')\n",
    "        mean += np.mean(img, axis=(0, 1))\n",
    "        std += np.std(img, axis=(0, 1))\n",
    "        num_images += 1\n",
    "    \n",
    "    # Calculate the mean and std across the dataset\n",
    "    mean /= num_images\n",
    "    std /= num_images\n",
    "    # print(\"HELLOOOOOOOOOOOOO\")\n",
    "    print(f\"Calculated Mean: {mean}\")\n",
    "    print(f\"Calculated Std: {std}\")\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "# temp_dataset = MimicCXR_Dataset(dataset, BASE_PATH, labels)\n",
    "# mean, std = temp_dataset.calculate_mean_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dcb7f1-848c-4c74-8da6-42084fc5294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean=0.47339121\n",
    "# std= 0.30462474\n",
    "# mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b775f42-83a7-4ca9-9a6e-8c8b34b86d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    CLAHETransform(clip_limit=0.34, tile_grid_size=(8, 8)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "validation_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    CLAHETransform(clip_limit=0.35, tile_grid_size=(8, 8)),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d81bbb-7f54-4711-9738-7ffa68ec27a1",
   "metadata": {},
   "source": [
    "## Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b9b01-257c-4ad5-900d-47c211569f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the frontal and lateral images (3-channel RGB)\n",
    "frontal_image_path = \"path_to_directory\"\n",
    "lateral_image_path = \"path_to_directory\"\n",
    "\n",
    "# Open and resize both images\n",
    "frontal_image = Image.open(frontal_image_path).convert(\"RGB\")\n",
    "frontal_image = frontal_image.resize((320, 320), Image.LANCZOS)\n",
    "\n",
    "lateral_image = Image.open(lateral_image_path).convert(\"RGB\")\n",
    "lateral_image = lateral_image.resize((320, 320), Image.LANCZOS)\n",
    "\n",
    "# Save both original images for visualization\n",
    "frontal_image.save('frontal_image_original.png')\n",
    "lateral_image.save('lateral_image_original.png')\n",
    "\n",
    "# Apply the transform directly to both PIL images\n",
    "frontal_transformed_image = validation_transform(frontal_image)\n",
    "lateral_transformed_image = validation_transform(lateral_image)\n",
    "\n",
    "# Convert back to numpy for visualization\n",
    "frontal_image_numpy = frontal_transformed_image.numpy().transpose((1, 2, 0))\n",
    "lateral_image_numpy = lateral_transformed_image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "# Unnormalize the images (reverse the normalization)\n",
    "mean = np.array([0.485, 0.456, 0.406])  # ImageNet mean for RGB\n",
    "std = np.array([0.229, 0.224, 0.225])   # ImageNet std for RGB\n",
    "\n",
    "frontal_unnormalized_image = frontal_image_numpy * std + mean\n",
    "frontal_unnormalized_image = np.clip(frontal_unnormalized_image, 0, 1)\n",
    "frontal_transformed_image_pil = Image.fromarray((frontal_unnormalized_image * 255).astype(np.uint8))\n",
    "\n",
    "lateral_unnormalized_image = lateral_image_numpy * std + mean\n",
    "lateral_unnormalized_image = np.clip(lateral_unnormalized_image, 0, 1)\n",
    "lateral_transformed_image_pil = Image.fromarray((lateral_unnormalized_image * 255).astype(np.uint8))\n",
    "\n",
    "# Save enhanced images\n",
    "frontal_transformed_image_pil.save('frontal_enhanced_image.png')\n",
    "lateral_transformed_image_pil.save('lateral_enhanced_image.png')\n",
    "\n",
    "# Plot the images\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "# Display the original frontal and lateral images\n",
    "axes[0, 0].imshow(frontal_image)\n",
    "axes[0, 0].set_title(\"Original Frontal Image\")\n",
    "\n",
    "axes[0, 1].imshow(lateral_image)\n",
    "axes[0, 1].set_title(\"Original Lateral Image\")\n",
    "\n",
    "# Display the enhanced frontal and lateral images\n",
    "axes[1, 0].imshow(frontal_transformed_image_pil)\n",
    "axes[1, 0].set_title(\"Enhanced Frontal Image\")\n",
    "\n",
    "axes[1, 1].imshow(lateral_transformed_image_pil)\n",
    "axes[1, 1].set_title(\"Enhanced Lateral Image\")\n",
    "\n",
    "# Save the comparison plot\n",
    "plt.savefig('frontal_lateral_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b428b5-5cd5-4dd7-b66b-c2f64fb5dcd7",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211b0083-7801-453f-9f6e-26abc600edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 50\n",
    "\n",
    "# # Create dataset instances\n",
    "# train_dataset = MimicCXR_Dataset(train_df, BASE_PATH, transform)\n",
    "# valid_dataset = MimicCXR_Dataset(valid_df, BASE_PATH, transform)\n",
    "# test_dataset = MimicCXR_Dataset(test_df, BASE_PATH, transform)\n",
    "\n",
    "# # Create data loaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=batch_size, pin_memory=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "# # Optional: Print data loader lengths\n",
    "# print(f'Train Loader Size: {len(train_loader)}')\n",
    "# print(f'Validation Loader Size: {len(valid_loader)}')\n",
    "# print(f'Test Loader Size: {len(test_loader)}')\n",
    "\n",
    "# Parameters\n",
    "batch_size = 50\n",
    "N_LABELS = 14\n",
    "start_epoch = 0\n",
    "num_epochs = 64  # Number of epochs to train for\n",
    "\n",
    "train_dataset = MimicCXR_Dataset(train_df, BASE_PATH, labels, transform=train_transform)\n",
    "valid_dataset = MimicCXR_Dataset(valid_df, BASE_PATH, labels, transform=validation_transform)\n",
    "test_dataset = MimicCXR_Dataset(test_df, BASE_PATH,labels, transform=validation_transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Optional: Print data loader lengths\n",
    "print(f'Train Loader Size: {len(train_loader)}')\n",
    "print(f'Validation Loader Size: {len(valid_loader)}')\n",
    "print(f'Test Loader Size: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508eee14-ab8a-4eb5-87e1-46e500ba40ba",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c39593-353d-4a9b-a5f3-e34945fa602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.densenet121 = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Identity()  # No classifier yet, only features\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.densenet121(x)\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.resnet50.fc.in_features\n",
    "        self.resnet50.fc = nn.Identity()  # No classifier yet, only features\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet50(x)\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, out_size):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        # Instantiate DenseNet121 and ResNet50 for frontal and lateral views\n",
    "        self.densenet_frontal = DenseNet121()\n",
    "        self.resnet_frontal = ResNet50()\n",
    "        \n",
    "        self.densenet_lateral = DenseNet121()\n",
    "        self.resnet_lateral = ResNet50()\n",
    "        \n",
    "        # The combined feature size\n",
    "        frontal_feature_size = 1024 + 2048  # Assuming DenseNet121 outputs 1024 and ResNet50 outputs 2048\n",
    "        lateral_feature_size = 1024 + 2048\n",
    "        \n",
    "        combined_feature_size = frontal_feature_size + lateral_feature_size\n",
    "        \n",
    "        # Final classifier layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_feature_size, out_size),\n",
    "            nn.Sigmoid()  # Assuming binary classification for multi-label\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_frontal, x_lateral):\n",
    "        # Extract features from DenseNet and ResNet for both views\n",
    "        frontal_features = torch.cat([self.densenet_frontal(x_frontal), self.resnet_frontal(x_frontal)], dim=1)\n",
    "        lateral_features = torch.cat([self.densenet_lateral(x_lateral), self.resnet_lateral(x_lateral)], dim=1)\n",
    "        \n",
    "        # Combine frontal and lateral features\n",
    "        combined_features = torch.cat([frontal_features, lateral_features], dim=1)\n",
    "        \n",
    "        # Final output through classifier\n",
    "        out = self.classifier(combined_features)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1559c1-5a13-485e-a0b1-0de37f7926fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LABELS = 14\n",
    "model = CombinedModel(N_LABELS).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416a665f-cd5e-41a0-83bb-1f923175edd8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1de8d06-2eae-4b37-9d02-9cfb385fb471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            \n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=50, threshold=0.5):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses, valid_losses = [], []\n",
    "    train_f1s, valid_f1s = [], []\n",
    "    train_precisions, valid_precisions = [], []\n",
    "    train_recalls, valid_recalls = [], []\n",
    "    train_accuracies, valid_accuracies = [], []\n",
    "\n",
    "    epoch_data = []\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=15, verbose=True)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        threshold=0.0001,\n",
    "        threshold_mode='abs'\n",
    "    )\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_labels, all_predictions = [], []\n",
    "\n",
    "        train_progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} [Train]', leave=True)\n",
    "        for (frontal_images, lateral_images), labels in train_progress_bar:\n",
    "            frontal_images, lateral_images, labels = frontal_images.to(device), lateral_images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(frontal_images, lateral_images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * frontal_images.size(0)\n",
    "            predictions = (outputs > threshold).float()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        train_f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "        train_precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "        train_recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "        train_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "        train_f1s.append(train_f1)\n",
    "        train_precisions.append(train_precision)\n",
    "        train_recalls.append(train_recall)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        valid_running_loss = 0.0\n",
    "        all_labels, all_predictions = [], []\n",
    "\n",
    "        valid_progress_bar = tqdm(valid_loader, desc=f'Epoch {epoch + 1}/{num_epochs} [Valid]', leave=True)\n",
    "        with torch.no_grad():\n",
    "            for (frontal_images, lateral_images), labels in valid_progress_bar:\n",
    "                frontal_images, lateral_images, labels = frontal_images.to(device), lateral_images.to(device), labels.to(device)\n",
    "                outputs = model(frontal_images, lateral_images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_running_loss += loss.item() * frontal_images.size(0)\n",
    "                predictions = (outputs > threshold).float()\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "        valid_epoch_loss = valid_running_loss / len(valid_loader.dataset)\n",
    "        valid_losses.append(valid_epoch_loss)\n",
    "\n",
    "        valid_f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "        valid_precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "        valid_recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "        valid_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "        valid_f1s.append(valid_f1)\n",
    "        valid_precisions.append(valid_precision)\n",
    "        valid_recalls.append(valid_recall)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print(f'Train - Loss: {epoch_loss:.4f}, F1: {train_f1:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, Accuracy: {train_accuracy:.4f}')\n",
    "        print(f'Valid - Loss: {valid_epoch_loss:.4f}, F1: {valid_f1:.4f}, Precision: {valid_precision:.4f}, Recall: {valid_recall:.4f}, Accuracy: {valid_accuracy:.4f}')\n",
    "\n",
    "        epoch_data.append([epoch + 1, epoch_loss, valid_epoch_loss, train_f1, valid_f1, train_precision, valid_precision, train_recall, valid_recall, train_accuracy, valid_accuracy])\n",
    "\n",
    "        if valid_epoch_loss < best_loss:\n",
    "            best_loss = valid_epoch_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(\"Model saved!\")\n",
    "\n",
    "        early_stopping(valid_epoch_loss)\n",
    "        scheduler.step(valid_epoch_loss)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 3 == 0:\n",
    "            plot_metrics(train_losses, valid_losses, train_f1s, valid_f1s, train_precisions, valid_precisions, \n",
    "                         train_recalls, valid_recalls, train_accuracies, valid_accuracies)\n",
    "\n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    all_labels, all_predictions = [], []\n",
    "    with torch.no_grad():\n",
    "        for (frontal_images, lateral_images), labels in valid_loader:\n",
    "            frontal_images, lateral_images, labels = frontal_images.to(device), lateral_images.to(device), labels.to(device)\n",
    "            outputs = model(frontal_images, lateral_images)\n",
    "            predictions = (outputs > threshold).float()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_predictions, zero_division=0))\n",
    "\n",
    "    metrics_df = pd.DataFrame(epoch_data, columns=[\n",
    "        'Epoch', 'Train Loss', 'Valid Loss', 'Train F1', 'Valid F1', \n",
    "        'Train Precision', 'Valid Precision', 'Train Recall', 'Valid Recall', \n",
    "        'Train Accuracy', 'Valid Accuracy'\n",
    "    ])\n",
    "    metrics_df.to_csv('epoch_metrics.csv', index=False)\n",
    "    print(\"Epoch data saved to epoch_metrics.csv\")\n",
    "\n",
    "    return train_losses, valid_losses, train_f1s, valid_f1s, train_precisions, valid_precisions, train_recalls, valid_recalls, train_accuracies, valid_accuracies\n",
    "\n",
    "def plot_metrics(train_losses, valid_losses, train_f1s, valid_f1s, train_precisions, valid_precisions, \n",
    "                 train_recalls, valid_recalls, train_accuracies, valid_accuracies):\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "    plt.plot(valid_losses, label='Validation Loss', color='orange')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "train_losses, valid_losses, train_f1s, valid_f1s, train_precisions, valid_precisions, train_recalls, valid_recalls, train_accuracies, valid_accuracies = train_model(model, train_loader, valid_loader, criterion, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
