{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de91d230-cdcb-40fb-811f-494ece80c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, precision_score, roc_curve, f1_score, auc, recall_score, accuracy_score, classification_report\n",
    "from transformers import ViTModel, ViTFeatureExtractor\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import json\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from skimage import exposure\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6bdeb-180e-4c37-8f5e-491ccee219c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90a82f1-2a5d-4027-a022-42dcffa663c8",
   "metadata": {},
   "source": [
    "## Implementation (CheXclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09265f27-45cc-4990-b6e8-a48a83fab9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"path_to_directory\"\n",
    "dataset = pd.read_csv(\"path_to_directory\")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c2122-2229-450a-8ca0-1184791cd8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dirs = ['p10', 'p11', 'p12', 'p13', 'p14', 'p15', 'p16', 'p17', 'p18', 'p19']\n",
    "\n",
    "# Function to check if the path format is correct\n",
    "def check_path_format(row, image_column):\n",
    "    image_path = os.path.normpath(row[image_column])\n",
    "    \n",
    "    # Split the path components\n",
    "    parts = image_path.split(os.sep)  # os.sep is platform-specific separator\n",
    "    \n",
    "    if len(parts) < 4:\n",
    "        return False\n",
    "    \n",
    "    # Check the main directory is valid\n",
    "    main_dir = parts[0]\n",
    "    if main_dir not in main_dirs:\n",
    "        return False\n",
    "    \n",
    "    subject_id = 'p' + str(row['subject_id'])\n",
    "    if parts[1] != subject_id:\n",
    "        return False\n",
    "    \n",
    "    study_id = 's' + str(row['study_id'])\n",
    "    if parts[2] != study_id:\n",
    "        return False\n",
    "    \n",
    "    # Check if file exists at the expected location\n",
    "    full_path = os.path.join(BASE_PATH, *parts)\n",
    "    if not os.path.exists(full_path):\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "dataset['frontal_image_valid'] = dataset.apply(lambda row: check_path_format(row, 'frontal_image'), axis=1)\n",
    "\n",
    "# Filter out rows with mismatches\n",
    "invalid_rows = dataset[(~dataset['frontal_image_valid'])]\n",
    "\n",
    "# Save invalid rows to a CSV file for inspection\n",
    "# invalid_rows.to_csv('invalid_image_paths.csv', index=False)\n",
    "print(f\"Number of invalid rows: {len(invalid_rows)}\")\n",
    "# print(\"Invalid rows saved to 'invalid_image_paths.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbec046c-01d2-4c6f-9547-3879091cee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion', 'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e84b9c-ccd7-433a-a2c2-d6441b9c36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_data = train_test_split(dataset, test_size=0.2, random_state=42)  # 80% training\n",
    "valid_df, test_df = train_test_split(temp_data, test_size=0.5, random_state=42)  # 10% validation, 10% testing\n",
    "\n",
    "# Display the lengths of the datasets\n",
    "print(f'Number of samples in training set: {len(train_df)}')\n",
    "print(f'Number of samples in validation set: {len(valid_df)}')\n",
    "print(f'Number of samples in test set: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b91f9b9-56a7-41be-b3a8-c8486ce06524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_leakage(df1, df2, study_col):\n",
    "    df1_studies_unique = set(df1[study_col].unique().tolist())\n",
    "    df2_studies_unique = set(df2[study_col].unique().tolist())\n",
    "    \n",
    "    # Check for any common studies in both datasets\n",
    "    studies_in_both_groups = df1_studies_unique.intersection(df2_studies_unique)\n",
    "    leakage = len(studies_in_both_groups) >= 1\n",
    "    return leakage\n",
    "\n",
    "# Check for leakage using the 'study_id' column\n",
    "print(\"Leakage between train and test: {}\".format(check_for_leakage(train_df, test_df, 'study_id')))\n",
    "print(\"Leakage between valid and test: {}\".format(check_for_leakage(valid_df, test_df, 'study_id')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac9e67-8325-478d-a1ba-a9e23832818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_paths(df):\n",
    "    image_paths = {\n",
    "        \"frontal_images\": df['frontal_image'].dropna().tolist(),\n",
    "    }\n",
    "    return image_paths\n",
    "\n",
    "# Extract image paths for each dataset\n",
    "train_image_paths = extract_image_paths(train_df)\n",
    "valid_image_paths = extract_image_paths(valid_df)\n",
    "test_image_paths = extract_image_paths(test_df)\n",
    "\n",
    "# Save each split to a JSON file\n",
    "def save_to_json(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Save the image paths\n",
    "\n",
    "save_to_json(train_image_paths, 'frontal_train.json')\n",
    "save_to_json(valid_image_paths, 'frontal_valid.json')\n",
    "save_to_json(test_image_paths, 'frontal_test.json')\n",
    "\n",
    "print(\"Image paths saved successfully in JSON format!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec413f8-cc25-493c-bdd3-26fc90e671e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_img = Image.open(\"path_to_directory\")\n",
    "img = np.asarray(pil_img).astype('uint8')\n",
    "print(img.max())\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f133410-9f6c-4371-84ba-8196371f5e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLAHETransform:\n",
    "    def __init__(self, clip_limit=0.10, tile_grid_size=(8, 8)):\n",
    "        self.clip_limit = clip_limit\n",
    "        self.tile_grid_size = tile_grid_size\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Convert PIL image to numpy array if necessary\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = np.array(img)\n",
    "        \n",
    "        # If the image is RGB (3 channels), convert it to LAB color space\n",
    "        if img.ndim == 3:\n",
    "            lab_img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "            l_channel, a_channel, b_channel = cv2.split(lab_img)\n",
    "\n",
    "            # Apply CLAHE only to the L (lightness) channel\n",
    "            l_channel = self.clahe.apply(l_channel)\n",
    "\n",
    "            # Merge back and convert to RGB\n",
    "            lab_img = cv2.merge((l_channel, a_channel, b_channel))\n",
    "            img = cv2.cvtColor(lab_img, cv2.COLOR_LAB2RGB)\n",
    "        else:\n",
    "            # If the image is grayscale, apply CLAHE directly\n",
    "            img = self.clahe.apply(img)\n",
    "\n",
    "        # Convert back to PIL Image before returning\n",
    "        return Image.fromarray(img.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b58d85-c852-4058-888f-2427e8370022",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MimicCXR_Dataset(Dataset):\n",
    "    def __init__(self, img_data, img_path, labels, transform=None):\n",
    "        self.img_data = img_data\n",
    "        self.img_path = img_path\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.img_data.iloc[index]['frontal_image']\n",
    "        img_name = os.path.join(self.img_path, str(img_name)) if pd.notna(img_name) else None\n",
    "        \n",
    "        if img_name is None or not os.path.exists(img_name):\n",
    "            raise FileNotFoundError(f'Image not found at path: {img_name}')\n",
    "    \n",
    "        image = Image.open(img_name).convert('RGB')  # Convert to 3-channel RGB\n",
    "        \n",
    "        # Fetch label, handle NaNs before converting to tensor\n",
    "        label = self.img_data.iloc[index][self.labels].fillna(0).values  # Replace NaNs with 0\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "        \n",
    "def calculate_mean_std(self):\n",
    "    # Temporary transform to convert images to tensors\n",
    "    # Initialize variables to store sums\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    num_images = 0\n",
    "    \n",
    "    # Iterate over all images\n",
    "    for img_file in self.img_data['frontal_image'].to_list():\n",
    "        img_path = os.path.join(self.img_path, img_file)\n",
    "        pil_img = Image.open(img_path)\n",
    "        img = np.asarray(pil_img).astype('uint8')\n",
    "        mean += np.mean(img, axis=(0, 1))\n",
    "        std += np.std(img, axis=(0, 1))\n",
    "        num_images += 1\n",
    "    \n",
    "    # Calculate the mean and std across the dataset\n",
    "    mean /= num_images\n",
    "    std /= num_images\n",
    "    # print(\"HELLOOOOOOOOOOOOO\")\n",
    "    print(f\"Calculated Mean: {mean}\")\n",
    "    print(f\"Calculated Std: {std}\")\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "# temp_dataset = MimicCXR_Dataset(dataset, BASE_PATH, labels)\n",
    "# mean, std = temp_dataset.calculate_mean_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dcb7f1-848c-4c74-8da6-42084fc5294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean=0.47339121\n",
    "# std= 0.30462474\n",
    "# mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b775f42-83a7-4ca9-9a6e-8c8b34b86d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize image to 256x256\n",
    "    CLAHETransform(clip_limit=0.34, tile_grid_size=(8, 8)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "validation_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    CLAHETransform(clip_limit=0.35, tile_grid_size=(8, 8)),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d81bbb-7f54-4711-9738-7ffa68ec27a1",
   "metadata": {},
   "source": [
    "## Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b9b01-257c-4ad5-900d-47c211569f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original image (3-channel RGB)\n",
    "image_path = \"path_to_directory\"\n",
    "original_image = Image.open(image_path).convert(\"RGB\")\n",
    "original_image = original_image.resize((320, 320), Image.LANCZOS)\n",
    "original_image.save('original_image.png')\n",
    "\n",
    "# Apply the transform directly to the PIL image (do not convert to NumPy yet)\n",
    "transformed_image = validation_transform(original_image)\n",
    "\n",
    "# Convert back to numpy for visualization\n",
    "image_numpy = transformed_image.numpy().transpose((1, 2, 0))  # No need to squeeze as we have 3 channels now\n",
    "\n",
    "# Unnormalize the image (reverse the normalization)\n",
    "mean = np.array([0.485, 0.456, 0.406])  # ImageNet mean for RGB\n",
    "std = np.array([0.229, 0.224, 0.225])   # ImageNet std for RGB\n",
    "unnormalized_image = image_numpy * std + mean\n",
    "unnormalized_image = np.clip(unnormalized_image, 0, 1)\n",
    "transformed_image_pil = Image.fromarray((unnormalized_image * 255).astype(np.uint8))\n",
    "transformed_image_pil.save('enhanced_image_another.png')\n",
    "\n",
    "# Plot the images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Display the original image\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].set_title(\"Original Image (RGB)\")\n",
    "\n",
    "# Display the enhanced image (CLAHE + resized)\n",
    "axes[1].imshow(transformed_image_pil)\n",
    "axes[1].set_title(\"Enhanced Image (CLAHE + Resized)\")\n",
    "\n",
    "# Save the comparison plot\n",
    "plt.savefig('comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b428b5-5cd5-4dd7-b66b-c2f64fb5dcd7",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211b0083-7801-453f-9f6e-26abc600edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 50\n",
    "\n",
    "# # Create dataset instances\n",
    "# train_dataset = MimicCXR_Dataset(train_df, BASE_PATH, transform)\n",
    "# valid_dataset = MimicCXR_Dataset(valid_df, BASE_PATH, transform)\n",
    "# test_dataset = MimicCXR_Dataset(test_df, BASE_PATH, transform)\n",
    "\n",
    "# # Create data loaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=batch_size, pin_memory=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "# # Optional: Print data loader lengths\n",
    "# print(f'Train Loader Size: {len(train_loader)}')\n",
    "# print(f'Validation Loader Size: {len(valid_loader)}')\n",
    "# print(f'Test Loader Size: {len(test_loader)}')\n",
    "\n",
    "# Parameters\n",
    "batch_size = 50\n",
    "N_LABELS = 14\n",
    "start_epoch = 0\n",
    "num_epochs = 64  # Number of epochs to train for\n",
    "\n",
    "train_dataset = MimicCXR_Dataset(train_df, BASE_PATH, labels, transform=train_transform)\n",
    "valid_dataset = MimicCXR_Dataset(valid_df, BASE_PATH, labels, transform=validation_transform)\n",
    "test_dataset = MimicCXR_Dataset(test_df, BASE_PATH,labels, transform=validation_transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Optional: Print data loader lengths\n",
    "print(f'Train Loader Size: {len(train_loader)}')\n",
    "print(f'Validation Loader Size: {len(valid_loader)}')\n",
    "print(f'Test Loader Size: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508eee14-ab8a-4eb5-87e1-46e500ba40ba",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c39593-353d-4a9b-a5f3-e34945fa602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DenseNetModel(nn.Module):\n",
    "#     def __init__(self, out_size):\n",
    "#         super(DenseNetModel, self).__init__()\n",
    "#         # self.base_model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
    "#         self.base_model = models.densenet121()\n",
    "#         self.base_model.features.conv0 = nn.Conv2d(\n",
    "#             in_channels=1,    # Set to 1 for grayscale images\n",
    "#             out_channels=64,   # Keep the original number of filters\n",
    "#             kernel_size=7, \n",
    "#             stride=2, \n",
    "#             padding=3, \n",
    "#             bias=False\n",
    "#         )\n",
    "#         num_ftrs = self.base_model.classifier.in_features\n",
    "#         self.base_model.classifier = nn.Sequential(\n",
    "#             nn.Linear(num_ftrs, out_size),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.base_model(x)\n",
    "#         return torch.sigmoid(x)\n",
    "\n",
    "class DenseNet121(nn.Module):\n",
    "    def __init__(self, out_size):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        # Use the latest ImageNet weights\n",
    "        self.densenet121 = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.densenet121(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1559c1-5a13-485e-a0b1-0de37f7926fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LABELS = 14\n",
    "model = DenseNet121(N_LABELS)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "# CriterionType = 'BCELoss' \n",
    "LR = 0.5e-3 \n",
    "criterion = nn.BCELoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416a665f-cd5e-41a0-83bb-1f923175edd8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1de8d06-2eae-4b37-9d02-9cfb385fb471",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=50, threshold=0.5):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Lists to store metrics\n",
    "    train_losses, valid_losses = [], []\n",
    "    train_f1s, valid_f1s = [], []\n",
    "    train_precisions, valid_precisions = [], []\n",
    "    train_recalls, valid_recalls = [], []\n",
    "    train_accuracies, valid_accuracies = [], []\n",
    "\n",
    "    # Prepare a list to save epoch data for CSV\n",
    "    epoch_data = []\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=15, verbose=True)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5,  # Reduce learning rate by a factor of 2\n",
    "        patience=3,  # Wait 3 epochs before reducing learning rate if no improvement\n",
    "        min_lr=1e-6,  # Minimum learning rate\n",
    "        threshold=0.0001,  # Threshold for measuring improvement\n",
    "        threshold_mode='abs'\n",
    "    )\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_labels, all_predictions = [], []\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} [Train]', leave=True)\n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            predictions = (outputs > threshold).float()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Calculate metrics for training set\n",
    "        train_f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "        train_precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "        train_recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "        train_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "        train_f1s.append(train_f1)\n",
    "        train_precisions.append(train_precision)\n",
    "        train_recalls.append(train_recall)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        valid_running_loss = 0.0\n",
    "        all_labels, all_predictions = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_running_loss += loss.item() * images.size(0)\n",
    "                predictions = (outputs > threshold).float()\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "        valid_epoch_loss = valid_running_loss / len(valid_loader.dataset)\n",
    "        valid_losses.append(valid_epoch_loss)\n",
    "\n",
    "        # Calculate metrics for validation set\n",
    "        valid_f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "        valid_precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "        valid_recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "        valid_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "        valid_f1s.append(valid_f1)\n",
    "        valid_precisions.append(valid_precision)\n",
    "        valid_recalls.append(valid_recall)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print(f'Train - Loss: {epoch_loss:.4f}, F1: {train_f1:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, Accuracy: {train_accuracy:.4f}')\n",
    "        print(f'Valid - Loss: {valid_epoch_loss:.4f}, F1: {valid_f1:.4f}, Precision: {valid_precision:.4f}, Recall: {valid_recall:.4f}, Accuracy: {valid_accuracy:.4f}')\n",
    "\n",
    "                # Save epoch data for CSV\n",
    "        epoch_data.append([epoch + 1, epoch_loss, valid_epoch_loss, train_f1, valid_f1, train_precision, valid_precision, train_recall, valid_recall, train_accuracy, valid_accuracy])\n",
    "\n",
    "        \n",
    "        if valid_epoch_loss < best_loss:\n",
    "            best_loss = valid_epoch_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(\"Model saved!\")\n",
    "\n",
    "        early_stopping(valid_epoch_loss)\n",
    "        scheduler.step(valid_epoch_loss)\n",
    "        optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 3 == 0:\n",
    "            plot_metrics(train_losses, valid_losses, train_f1s, valid_f1s, train_precisions, valid_precisions, \n",
    "                         train_recalls, valid_recalls, train_accuracies, valid_accuracies)\n",
    "\n",
    "    # Classification report at the end\n",
    "    model.eval()\n",
    "    all_labels, all_predictions = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predictions = (outputs > threshold).float()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_predictions, zero_division=0))\n",
    "\n",
    "    # Save epoch metrics to CSV\n",
    "    metrics_df = pd.DataFrame(epoch_data, columns=[\n",
    "        'Epoch', 'Train Loss', 'Valid Loss', 'Train F1', 'Valid F1', \n",
    "        'Train Precision', 'Valid Precision', 'Train Recall', 'Valid Recall', \n",
    "        'Train Accuracy', 'Valid Accuracy'\n",
    "    ])\n",
    "    metrics_df.to_csv('epoch_metrics.csv', index=False)\n",
    "    print(\"Epoch data saved to epoch_metrics.csv\")\n",
    "\n",
    "    return train_losses, valid_losses, train_f1s, valid_f1s, train_precisions, valid_precisions, train_recalls, valid_recalls, train_accuracies, valid_accuracies\n",
    "\n",
    "def plot_metrics(train_losses, valid_losses, train_f1s, valid_f1s, train_precisions, valid_precisions, \n",
    "                 train_recalls, valid_recalls, train_accuracies, valid_accuracies):\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "    plt.plot(valid_losses, label='Validation Loss', color='orange')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "train_losses, valid_losses, train_f1s, valid_f1s, train_precisions, valid_precisions, train_recalls, valid_recalls, train_accuracies, valid_accuracies = train_model(model, train_loader, valid_loader, criterion, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
