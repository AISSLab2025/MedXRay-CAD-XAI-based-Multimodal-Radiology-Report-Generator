{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644d7bd-185a-45f6-8ebb-ba9ec3a5a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torchvision import models, transforms\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import requests\n",
    "from pytorch_grad_cam import GradCAMPlusPlus, GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    deprocess_image, \\\n",
    "    preprocess_image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f4e8f-b2d9-47fc-a70c-2438811263ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1980580-8488-4890-b492-398e6e880b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CLAHE Transform class\n",
    "class CLAHETransform:\n",
    "    def __init__(self, clip_limit=0.10, tile_grid_size=(8, 8)):\n",
    "        self.clip_limit = clip_limit\n",
    "        self.tile_grid_size = tile_grid_size\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Convert PIL image to numpy array if necessary\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = np.array(img)\n",
    "        \n",
    "        # If the image is RGB (3 channels), convert it to LAB color space\n",
    "        if img.ndim == 3:\n",
    "            lab_img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "            l_channel, a_channel, b_channel = cv2.split(lab_img)\n",
    "\n",
    "            # Apply CLAHE only to the L (lightness) channel\n",
    "            l_channel = self.clahe.apply(l_channel)\n",
    "\n",
    "            # Merge back and convert to RGB\n",
    "            lab_img = cv2.merge((l_channel, a_channel, b_channel))\n",
    "            img = cv2.cvtColor(lab_img, cv2.COLOR_LAB2RGB)\n",
    "        else:\n",
    "            # If the image is grayscale, apply CLAHE directly\n",
    "            img = self.clahe.apply(img)\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d4247a-a461-45fd-800f-52ee22783cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    def __init__(self, out_size):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        # Use the latest ImageNet weights\n",
    "        self.densenet121 = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.densenet121(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096feee2-2d5d-442d-8c4f-26cd296ef3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LABELS = 14\n",
    "# Load the saved model\n",
    "best_model = DenseNet121(out_size=N_LABELS)\n",
    "best_model.load_state_dict(torch.load('path_to_model.pth'))\n",
    "best_model = best_model.to(device)\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6923b1-ad35-4a72-858b-e54d2b0ce5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the image for DenseNet121\n",
    "image_url = \"path_to_directory\"\n",
    "img = np.array(Image.open(image_url))\n",
    "clahe_transform = CLAHETransform(clip_limit=0.34, tile_grid_size=(8, 8))\n",
    "img = clahe_transform(img)\n",
    "img = cv2.resize(img, (256, 256))\n",
    "img = np.float32(img) / 255\n",
    "img = np.stack([img] * 3, axis=-1)\n",
    "input_tensor = torch.from_numpy(img[np.newaxis, ...]).permute(0, 3, 1, 2)\n",
    "\n",
    "targets = [ClassifierOutputTarget(13)]\n",
    "target_layers = [best_model.densenet121.features.denseblock4]\n",
    "with GradCAMPlusPlus(model=best_model, target_layers=target_layers) as cam:\n",
    "    grayscale_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "    cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "\n",
    "# Convert the grayscale CAM to a three-channel image\n",
    "cam = np.uint8(255 * grayscale_cams[0, :])\n",
    "cam = cv2.applyColorMap(cam, cv2.COLORMAP_TWILIGHT_SHIFTED)  # Apply color map for better visualization\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Original Image\n",
    "axes[0].imshow(img)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('(a) Original Image')\n",
    "\n",
    "# CAM Heatmap\n",
    "axes[1].imshow(cam, cmap='plasma')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('(b) Grad-CAM Heatmap')\n",
    "\n",
    "# Overlay Image\n",
    "axes[2].imshow(cam_image)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('(c) Overlay Image')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the final figure\n",
    "# Save the overlay image\n",
    "overlay_image_path =  \"path_to_save_directory\"\n",
    "cv2.imwrite(overlay_image_path, cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778caba0-cfee-4df7-976c-cc389c06c99f",
   "metadata": {},
   "source": [
    "##### Lateral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee3391-93b3-4c7c-8f65-cc283227b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, out_size):\n",
    "        super(ResNet50, self).__init__()\n",
    "        # Use the latest ImageNet weights for ResNet50\n",
    "        self.resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.resnet50.fc.in_features\n",
    "        self.resnet50.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "            nn.Sigmoid()  # Assuming you're doing a binary classification, adjust as needed\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet50(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e8efd-a9f5-4e46-8292-473879778f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LABELS = 14\n",
    "# Load the saved model\n",
    "lateral_model = ResNet50(out_size=N_LABELS)\n",
    "lateral_model.load_state_dict(torch.load('path_to_best_model.pth'))\n",
    "lateral_model = lateral_model.to(device)\n",
    "lateral_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121013e9-ae67-404a-bae7-fdc1bb2d314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the image for ResNet50\n",
    "image_url = \"path_to_directory\"\n",
    "img = Image.open(image_url)\n",
    "clahe_transform = CLAHETransform(clip_limit=0.34, tile_grid_size=(8, 8))\n",
    "img = clahe_transform(img)\n",
    "img = cv2.resize(img, (256, 256))\n",
    "img = np.float32(img) / 255\n",
    "if img.ndim == 2:  # If grayscale, convert to RGB\n",
    "    img = np.stack([img] * 3, axis=-1)\n",
    "input_tensor = torch.from_numpy(img[np.newaxis, ...]).permute(0, 3, 1, 2).to(device)\n",
    "\n",
    "# Define the target layer and targets for ResNet50\n",
    "targets = [ClassifierOutputTarget(13)]  # Replace 13 with the target class index as needed\n",
    "target_layers = [lateral_model.resnet50.layer4[-1]]\n",
    "\n",
    "# Use GradCAM++ to generate the visualization for ResNet50\n",
    "with GradCAMPlusPlus(model=lateral_model, target_layers=target_layers) as cam:\n",
    "    grayscale_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "    cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "\n",
    "# Convert the grayscale CAM to a three-channel image\n",
    "cam = np.uint8(255 * grayscale_cams[0, :])\n",
    "cam = cv2.applyColorMap(cam, cv2.COLORMAP_TWILIGHT_SHIFTED)  # Apply color map for better visualization\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Original Image\n",
    "axes[0].imshow(img)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('(a) Original Image')\n",
    "\n",
    "# CAM Heatmap\n",
    "axes[1].imshow(cam, cmap='plasma')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('(b) Grad-CAM Heatmap')\n",
    "\n",
    "# Overlay Image\n",
    "axes[2].imshow(cam_image)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('(c) Overlay Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the final figure\n",
    "fig.savefig(\"file_name.png\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "overlay_image_path = \"path_to_save_directory\"\n",
    "cv2.imwrite(overlay_image_path, cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
