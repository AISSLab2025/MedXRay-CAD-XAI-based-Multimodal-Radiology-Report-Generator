{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b053dd-f754-4096-be1d-9a33df9bb57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# RAG\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOllama\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from adjustText import adjust_text\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import densenet121\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import csv\n",
    "import clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f198f-66fc-4d30-8ee1-e825daa6af07",
   "metadata": {},
   "source": [
    "## Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e7f8a-1621-4987-abe6-96d90fc947e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels as used during training\n",
    "labels = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', \n",
    "          'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion', 'Lung Opacity', \n",
    "          'No Finding', 'Pleural Effusion', 'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices']\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, out_size=14):\n",
    "        super(ResNet50, self).__init__()\n",
    "        # Use the latest ImageNet weights for ResNet50\n",
    "        self.resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.resnet50.fc.in_features\n",
    "        self.resnet50.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "            nn.Sigmoid()  # Assuming you're doing a binary classification, adjust as needed\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet50(x)\n",
    "        \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNet50().to(device)\n",
    "model.load_state_dict(torch.load('best_save_models/lateral_best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# CLAHE transform class\n",
    "class CLAHETransform:\n",
    "    def __init__(self, clip_limit=0.10, tile_grid_size=(8, 8)):\n",
    "        self.clip_limit = clip_limit\n",
    "        self.tile_grid_size = tile_grid_size\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = np.array(img)\n",
    "        if img.ndim == 3:\n",
    "            lab_img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "            l_channel, a_channel, b_channel = cv2.split(lab_img)\n",
    "            l_channel = self.clahe.apply(l_channel)\n",
    "            lab_img = cv2.merge((l_channel, a_channel, b_channel))\n",
    "            img = cv2.cvtColor(lab_img, cv2.COLOR_LAB2RGB)\n",
    "        else:\n",
    "            img = self.clahe.apply(img)\n",
    "        return Image.fromarray(img.astype('uint8'))\n",
    "\n",
    "# Define the validation transform (same as during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    CLAHETransform(clip_limit=0.35, tile_grid_size=(8, 8)),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# Function to predict an image\n",
    "def predict_image(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "    \n",
    "    # Convert predictions to numpy array and map to labels\n",
    "    predictions = output.cpu().numpy().squeeze()  # Remove batch dimension\n",
    "    pred_scores = {labels[i]: predictions[i] for i in range(len(predictions))}\n",
    "    \n",
    "    return pred_scores\n",
    "\n",
    "# # Example usage\n",
    "# image_path = 'text_reports/testing/IMAGES/p13952691_s54551451.jpg' # Real labels for this image {Consolidation, Pleural Effusion, Supporting Device}\n",
    "# pred_scores = predict_image(image_path)\n",
    "# # pred_scores\n",
    "# # Print the scores for each label\n",
    "# for label, score in pred_scores.items():\n",
    "#     print(f'{label}: {score:.4f}')\n",
    "\n",
    "def get_top_classifications(classifications: dict, top_n: int = 3, threshold: float = 0.50) -> dict:\n",
    "    # Filter classifications that are above the threshold\n",
    "    filtered_classifications = {k: v for k, v in classifications.items() if v > threshold}\n",
    "    \n",
    "    # Sort the filtered classifications by their score in descending order\n",
    "    sorted_classifications = sorted(filtered_classifications.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    # Return the top N classifications\n",
    "    top_classifications = {k: v for k, v in sorted_classifications[:top_n]}\n",
    "    \n",
    "    return top_classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd4f2c-9504-4d66-81fb-07820787379f",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0ac92-e43b-4d46-a698-fee5c8f0f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the embedding model and vector database\n",
    "oembed = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "vectordb = Chroma(\n",
    "    persist_directory=\"text_reports/all_embed_db\",\n",
    "    embedding_function=oembed\n",
    ")\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "ollama_llm = ChatOllama(base_url=\"http://localhost:11434\", model=\"llava\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64504ea-6107-49e8-a7ab-33162ca3b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_report = \"\"\"\n",
    "EXAMINATION: CHEST (PA AND LAT)\n",
    "\n",
    "INDICATION: ___F with new onset ascites\n",
    "\n",
    "TECHNIQUE: Chest PA and lateral\n",
    "\n",
    "COMPARISON: None.\n",
    "\n",
    "FINDINGS: \n",
    "There is no focal consolidation, pleural effusion or pneumothorax. Bilateral nodular opacities that most likely represent nipple shadows. The cardiomediastinal silhouette is normal. Clips project over the left lung, potentially within the breast. The imaged upper abdomen is unremarkable. Chronic deformity of the posterior left sixth and seventh ribs are noted.\n",
    "\n",
    "IMPRESSION: \n",
    "No acute cardiopulmonary process.\n",
    "\"\"\"\n",
    "\n",
    "# Define the report template\n",
    "report_template = \"\"\"\n",
    "1. EXAMINATION: {examination}\n",
    "2. INDICATION: {indication}\n",
    "3. TECHNIQUE: {technique}\n",
    "4. COMPARISON: {comparison}\n",
    "5. FINDINGS: {findings}\n",
    "6. IMPRESSION: {impression}\n",
    "\"\"\"\n",
    "\n",
    "# Set up the prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are an expert radiologist. Generate a concise and accurate radiology report based on the provided context and classification. Follow these strict guidelines:\n",
    "    \n",
    "    1. Use ONLY the given template structure. Do not add any sections or text outside this structure.\n",
    "    2. Keep EXAMINATION, INDICATION, TECHNIQUE, and COMPARISON brief, as link sample report.\n",
    "    3. FINDINGS should be detailed but focused, describing only relevant observations.\n",
    "    4. IMPRESSION should summarize key findings and their clinical significance concisely.\n",
    "    5. Do not include any introductory or concluding statements, notifications, or recommendations unless explicitly part of the findings or impression.\n",
    "    6. Use appropriate medical terminology and maintain a professional tone.\n",
    "    7. Base your report solely on the given context and classification.\n",
    "    \n",
    "    Classification: {classifications}\n",
    "    Context: {context}\n",
    "    Query: {query}\n",
    "\n",
    "    Generate the report now, strictly following the template below:\n",
    "    {report_template}\n",
    "\n",
    "    Here's a sample report for reference:\n",
    "    {sample_report}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a97dfe-ce0f-4d9b-97df-7fb8fb860f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLMChain with the prompt template and Ollama LLM\n",
    "llm_chain = LLMChain(prompt=prompt_template, llm=ollama_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eed7b3-9274-4e88-be41-4e8375ebadb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP model and preprocessing function\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ba8d2-5d86-4da2-9e8c-648af13b65dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pad embeddings to the required dimension (1024)\n",
    "def embed_query_image_and_caption(query, image_path, image_caption, target_dim=1024):\n",
    "    # Embed the query text using CLIP\n",
    "    text = clip.tokenize([query]).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_embedding = clip_model.encode_text(text).cpu().numpy()\n",
    "\n",
    "    # Embed the image using CLIP\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_embedding = clip_model.encode_image(image).cpu().numpy()\n",
    "\n",
    "    # Embed the image caption using Ollama\n",
    "    caption_embedding = np.array(oembed.embed_query(image_caption))\n",
    "\n",
    "    # Pad CLIP embeddings to match Ollama embedding size\n",
    "    query_embedding_padded = pad_embedding(query_embedding, target_dim)\n",
    "    image_embedding_padded = pad_embedding(image_embedding, target_dim)\n",
    "\n",
    "    # Combine embeddings (you can experiment with different combination methods)\n",
    "    combined_embedding = (query_embedding_padded + image_embedding_padded + caption_embedding.reshape(1, -1)) / 3\n",
    "\n",
    "    return combined_embedding.flatten().tolist()  # Convert to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8f59f-ff3d-4298-bf3c-8dae40ca28d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the pad_embedding function to handle 2D arrays\n",
    "def pad_embedding(embedding, target_dim=1024):\n",
    "    if embedding.shape[1] < target_dim:\n",
    "        padding = np.zeros((embedding.shape[0], target_dim - embedding.shape[1]))\n",
    "        return np.hstack((embedding, padding))\n",
    "    elif embedding.shape[1] > target_dim:\n",
    "        return embedding[:, :target_dim]\n",
    "    return embedding\n",
    "\n",
    "# Function to retrieve similar documents\n",
    "def retrieve_similar_documents(combined_embedding, k=5):\n",
    "    return vectordb.similarity_search_by_vector(combined_embedding, k=k)\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def calculate_cosine_similarity(query_embedding, doc_embeddings):\n",
    "    query_embedding = np.array(query_embedding).reshape(1, -1)\n",
    "    doc_embeddings = np.array(doc_embeddings)\n",
    "    cosine_similarities = cosine_similarity(query_embedding, doc_embeddings).flatten()\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce4a265-47af-4453-80e4-22f39ccf9c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the medical report\n",
    "def generate_medical_report(classifications, image_path, image_caption):\n",
    "    # Construct a more detailed query\n",
    "    query = (\n",
    "        f\"Examine the chest X-ray for the presence of {', '.join(classifications)}. \"\n",
    "        \"Provide a comprehensive analysis of the observed radiographic features and their clinical implications.\"\n",
    "    )\n",
    "\n",
    "    # Combine query, image, and image caption embeddings\n",
    "    combined_embedding = embed_query_image_and_caption(query, image_path, image_caption)\n",
    "    \n",
    "    # Retrieve similar documents using the combined embedding\n",
    "    similar_docs = retrieve_similar_documents(combined_embedding)\n",
    "    \n",
    "    # Extract the content from similar documents\n",
    "    context = \"\\n\".join([doc.page_content for doc in similar_docs])\n",
    "    \n",
    "    # Show the retrieved documents\n",
    "    print(\"Retrieved Documents:\")\n",
    "    for i, doc in enumerate(similar_docs, 1):\n",
    "        print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "    \n",
    "    # Run the LLM chain with context, query, classifications, and image caption\n",
    "    result = llm_chain.run({\n",
    "        \"context\": context,\n",
    "        \"query\": query,\n",
    "        \"classifications\": \", \".join(classifications),\n",
    "        \"sample_report\": sample_report,\n",
    "        \"image_caption\": image_caption,\n",
    "        \"report_template\": report_template\n",
    "    })\n",
    "\n",
    "    # Parse the output for clean presentation\n",
    "    parsed_result = StrOutputParser().parse(result)\n",
    "    \n",
    "    print(\"Generated Medical Report:\")\n",
    "    print(parsed_result)\n",
    "    \n",
    "    # Visualize the query, image caption, and similarity of the retrieved documents\n",
    "    # visualize_embeddings(query, image_caption, similar_docs, combined_embedding)\n",
    "    \n",
    "    return parsed_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3aeb47-e8cf-492f-bfe6-c2b8e9dacc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_caption_from_report(report_path):\n",
    "    \"\"\"\n",
    "    Extract the caption from the report by searching for FINDINGS: or IMPRESSION:.\n",
    "    Falls back to a default caption if neither is found.\n",
    "    \"\"\"\n",
    "    default_caption = \"Detailed frontal chest X-ray showcasing the lungs, heart, and adjacent thoracic structures, providing a clear view for medical analysis.\"\n",
    "    \n",
    "    try:\n",
    "        with open(report_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            \n",
    "            # Search for FINDINGS: or IMPRESSION:\n",
    "            findings_index = content.find('FINDINGS:')\n",
    "            if findings_index != -1:\n",
    "                return content[findings_index + 9:].strip()  # Extract text after FINDINGS:\n",
    "\n",
    "            impression_index = content.find('IMPRESSION:')\n",
    "            if impression_index != -1:\n",
    "                return content[impression_index + 11:].strip()  # Extract text after IMPRESSION:\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Report file not found: {report_path}\")\n",
    "    \n",
    "    # Return default caption if no keywords found or file is missing\n",
    "    return default_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e892898-11f3-4004-a944-def1a7b36360",
   "metadata": {},
   "source": [
    "## llava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29de87-f9e8-4cfb-b5d1-78dabd0439ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_directory(image_dir, report_csv, report_dir):\n",
    "    \"\"\"\n",
    "    Process images in the directory, generate medical reports, and save the results to a CSV.\n",
    "    \"\"\"\n",
    "    # Prepare the output CSV\n",
    "    with open(report_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['image_name', 'classifications', 'generated_report', 'original_report', 'caption'])\n",
    "\n",
    "        # Process each image in the directory\n",
    "        for image_name in os.listdir(image_dir):\n",
    "            image_path = os.path.join(image_dir, image_name)\n",
    "            \n",
    "            if os.path.isfile(image_path):  # Ensure it's a valid file\n",
    "                # Retrieve the original report and caption\n",
    "                report_filename = image_name.rsplit('.', 1)[0] + '.txt'\n",
    "                original_report_path = os.path.join(report_dir, report_filename)\n",
    "                caption = extract_caption_from_report(original_report_path)\n",
    "                \n",
    "                # Get predictions and classifications\n",
    "                pred_score = predict_image(image_path)\n",
    "                for label, score in pred_score.items():\n",
    "                    print(f'{label}: {score:.4f}')\n",
    "                classifications = get_top_classifications(pred_score)\n",
    "                \n",
    "                print(f\"\\nTop classification: {classifications}. Image Caption: {caption}\\n\")\n",
    "\n",
    "                # Generate a medical report\n",
    "                generated_report = generate_medical_report(classifications, image_path, caption)\n",
    "\n",
    "                # Read the original report\n",
    "                original_report = ''\n",
    "                try:\n",
    "                    with open(original_report_path, 'r', encoding='utf-8') as file:\n",
    "                        original_report = file.read()\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"Warning: Original report not found for {report_filename}\")\n",
    "\n",
    "                # Save results to the CSV\n",
    "                writer.writerow([\n",
    "                    image_name,\n",
    "                    classifications,\n",
    "                    generated_report,\n",
    "                    original_report,\n",
    "                    caption\n",
    "                ])\n",
    "\n",
    "                print(f\"Processed: {image_name}\")\n",
    "    \n",
    "    print(f\"Processing complete. Results saved to {report_csv}\")\n",
    "\n",
    "# Paths\n",
    "image_dir = 'path_to_directory'\n",
    "report_dir = 'path_to_directory'\n",
    "report_csv = 'path_to_directory'\n",
    "\n",
    "# Process directory\n",
    "process_directory(image_dir, report_csv, report_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
