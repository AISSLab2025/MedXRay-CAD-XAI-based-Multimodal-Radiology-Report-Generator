{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c96a3d-2645-499f-94d3-018cb0a4b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report\n",
    "import wandb\n",
    "import cv2\n",
    "from torchvision.models import ViT_B_16_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc519a-1f79-4d79-ab90-117a7cb1c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3504bc9-e70d-4df1-bd83-ca5d45a4d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = 'path_to_directory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49377aff-6eeb-4621-a2e7-b645e4a08d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"path_to_directory\")\n",
    "\n",
    "labels = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', \n",
    "          'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion', 'Lung Opacity', \n",
    "          'No Finding', 'Pleural Effusion', 'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4668bd15-1a84-4b70-9386-0c21b4d09ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_data = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "valid_df, test_df = train_test_split(temp_data, test_size=(1/3), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a8783-1385-4fbe-9968-765a69d9bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLAHETransform:\n",
    "    def __init__(self, clip_limit=0.34, tile_grid_size=(8, 8)):\n",
    "        self.clip_limit = clip_limit\n",
    "        self.tile_grid_size = tile_grid_size\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = np.array(img)\n",
    "        if img.ndim == 3:\n",
    "            lab_img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "            l_channel, a_channel, b_channel = cv2.split(lab_img)\n",
    "            l_channel = self.clahe.apply(l_channel)\n",
    "            lab_img = cv2.merge((l_channel, a_channel, b_channel))\n",
    "            img = cv2.cvtColor(lab_img, cv2.COLOR_LAB2RGB)\n",
    "        else:\n",
    "            img = self.clahe.apply(img)\n",
    "        img = np.clip(img, 0, 255)\n",
    "        return Image.fromarray(img.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c38b9c-e728-46ce-8f98-463976b04018",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    CLAHETransform(clip_limit=0.34, tile_grid_size=(8, 8)),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914956e2-3afe-447a-a105-780f738caed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MimicCXR_Dataset(Dataset):\n",
    "    def __init__(self, img_data, img_path, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.transform = transform\n",
    "        self.img_data = img_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.img_data.iloc[index]['frontal_image']\n",
    "        if pd.isna(img_name):\n",
    "            raise ValueError(f'Missing image path for index {index}')\n",
    "        img_name = os.path.join(self.img_path, str(img_name))\n",
    "        try:\n",
    "            image = Image.open(img_name).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f'Image not found at path: {img_name}')\n",
    "        label = torch.zeros(len(labels), dtype=torch.float32)\n",
    "        for i, col in enumerate(labels):\n",
    "            label[i] = self.img_data.iloc[index][col]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16dcc5-ec58-489e-9043-3369ea058f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, path='path_to_model.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607cbe58-b3aa-4753-b146-f73c06274c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNetModel, self).__init__()\n",
    "        self.base_model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
    "        num_ftrs = self.base_model.classifier.in_features\n",
    "        self.base_model.classifier = nn.Linear(num_ftrs, len(labels))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetModel, self).__init__()\n",
    "        self.base_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        num_ftrs = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Linear(num_ftrs, len(labels))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class EfficientNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EfficientNetModel, self).__init__()\n",
    "        self.base_model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        num_ftrs = self.base_model.classifier[1].in_features\n",
    "        self.base_model.classifier[1] = nn.Linear(num_ftrs, len(labels))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class InceptionV3Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InceptionV3Model, self).__init__()\n",
    "        self.base_model = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "        num_ftrs = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Linear(num_ftrs, len(labels))\n",
    "        self.base_model.aux_logits = False  # Disable auxiliary output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Inception v3 expects (299,299) sized images\n",
    "        if x.size()[2:] != (299, 299):\n",
    "            x = nn.functional.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        x = self.base_model(x)\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Inception v3 expects (299,299) sized images\n",
    "        if x.size()[2:] != (299, 299):\n",
    "            x = nn.functional.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        x = self.base_model(x)\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]  # In training, inception returns (output, aux_output)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class VGG16Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16Model, self).__init__()\n",
    "        self.base_model = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "        num_ftrs = self.base_model.classifier[6].in_features\n",
    "        self.base_model.classifier[6] = nn.Linear(num_ftrs, len(labels))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class DenseNet201Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet201Model, self).__init__()\n",
    "        self.base_model = models.densenet201(weights=models.DenseNet201_Weights.DEFAULT)\n",
    "        num_ftrs = self.base_model.classifier.in_features\n",
    "        self.base_model.classifier = nn.Linear(num_ftrs, len(labels))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class ResNeXt101Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNeXt101Model, self).__init__()\n",
    "        self.base_model = models.resnext101_32x8d(weights=models.ResNeXt101_32X8D_Weights.DEFAULT)\n",
    "        num_ftrs = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Linear(num_ftrs, len(labels))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class EfficientNetB4Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EfficientNetB4Model, self).__init__()\n",
    "        self.base_model = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.DEFAULT)\n",
    "        num_ftrs = self.base_model.classifier[1].in_features\n",
    "        self.base_model.classifier[1] = nn.Linear(num_ftrs, len(labels))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class VisionTransformerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VisionTransformerModel, self).__init__()\n",
    "        self.base_model = models.vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "        num_ftrs = self.base_model.heads.head.in_features\n",
    "        self.base_model.heads.head = nn.Linear(num_ftrs, len(labels))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # ViT expects images of size 224x224\n",
    "        if x.size()[2:] != (224, 224):\n",
    "            x = nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        x = self.base_model(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49e5d85-e2b1-4d49-8d67-fcf2651dc7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Initialize wandb\n",
    "    wandb.init()\n",
    "    \n",
    "    # Access sweep parameters\n",
    "    config = wandb.config\n",
    "    \n",
    "    # Data loaders\n",
    "    train_dataset = MimicCXR_Dataset(train_df, BASE_PATH, transform)\n",
    "    valid_dataset = MimicCXR_Dataset(valid_df, BASE_PATH, transform)\n",
    "    test_dataset = MimicCXR_Dataset(test_df, BASE_PATH, transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, drop_last=True)\n",
    "\n",
    "    # Model setup\n",
    "    if config.architecture == 'densenet121':\n",
    "        model = DenseNetModel().to(device)\n",
    "    elif config.architecture == 'resnet50':\n",
    "        model = ResNetModel().to(device)\n",
    "    elif config.architecture == 'efficientnet':\n",
    "        model = EfficientNetModel().to(device)\n",
    "    elif config.architecture == 'inceptionv3':\n",
    "        model = InceptionV3Model().to(device)\n",
    "    elif config.architecture == 'vgg16':\n",
    "        model = VGG16Model().to(device)\n",
    "    elif config.architecture == 'densenet201':\n",
    "        model = DenseNet201Model().to(device)\n",
    "    elif config.architecture == 'resnext101':\n",
    "        model = ResNeXt101Model().to(device)\n",
    "    elif config.architecture == 'efficientnet_b4':\n",
    "        model = EfficientNetB4Model().to(device)\n",
    "    elif config.architecture == 'vit':\n",
    "        model = VisionTransformerModel().to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Optimizer setup\n",
    "    if config.optimizer == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    elif config.optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9, weight_decay=config.weight_decay)\n",
    "    elif config.optimizer == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    elif config.optimizer == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    if config.lr_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config.lr_step_size, gamma=config.lr_gamma)\n",
    "    elif config.lr_scheduler == 'cosine':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True, path=f'hyperparameter_best_models/best_model_{wandb.run.id}.pt')\n",
    "    \n",
    "    # Log model architecture and hyperparameters\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    wandb.log({\n",
    "        \"batch_size\": config.batch_size,\n",
    "        \"architecture\": config.architecture,\n",
    "        \"optimizer\": config.optimizer,\n",
    "        \"learning_rate\": config.learning_rate,\n",
    "        \"weight_decay\": config.weight_decay,\n",
    "        \"lr_scheduler\": config.lr_scheduler,\n",
    "    })\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            if data.size(0) < 2:  # Skip batches smaller than 2\n",
    "                continue\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_outputs = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in valid_loader:\n",
    "                if data.size(0) < 2:  # Skip batches smaller than 2\n",
    "                    continue\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss += loss.item()\n",
    "                val_outputs.append(outputs.cpu().numpy())\n",
    "                val_targets.append(target.cpu().numpy())\n",
    "        \n",
    "        val_loss = val_loss / len(valid_loader)\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_outputs = np.concatenate(val_outputs)\n",
    "        val_targets = np.concatenate(val_targets)\n",
    "        val_preds = (val_outputs > 0.5).astype(int)\n",
    "        val_precision = precision_score(val_targets, val_preds, average='macro', zero_division=1)\n",
    "        val_recall = recall_score(val_targets, val_preds, average='macro', zero_division=1)\n",
    "        val_accuracy = accuracy_score(val_targets, val_preds)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_precision\": val_precision,\n",
    "            \"val_recall\": val_recall,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "        })\n",
    "        \n",
    "        print(f'Epoch {epoch}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, '\n",
    "              f'Validation Precision: {val_precision:.4f}, Validation Recall: {val_recall:.4f}, '\n",
    "              f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(f'hyperparameter_best_models/best_model_{wandb.run.id}.pt'))\n",
    "    \n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if data.size(0) < 2:  # Skip batches smaller than 2\n",
    "                continue\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "            all_targets.append(target.cpu().numpy())\n",
    "\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "\n",
    "    # Calculate and log test metrics\n",
    "    test_preds = (all_outputs > 0.5).astype(int)\n",
    "    test_precision = precision_score(all_targets, test_preds, average='macro', zero_division=1)\n",
    "    test_recall = recall_score(all_targets, test_preds, average='macro', zero_division=1)\n",
    "    test_accuracy = accuracy_score(all_targets, test_preds)\n",
    "\n",
    "    wandb.log({\n",
    "        \"test_precision\": test_precision,\n",
    "        \"test_recall\": test_recall,\n",
    "        \"test_accuracy\": test_accuracy\n",
    "    })\n",
    "\n",
    "    # Log classification report to wandb\n",
    "    report = classification_report(all_targets, test_preds, target_names=labels, output_dict=True, zero_division=1)\n",
    "    wandb.log({\"classification_report\": wandb.Table(dataframe=pd.DataFrame(report).transpose())})\n",
    "\n",
    "    # Log confusion matrix\n",
    "    wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "        probs=None,\n",
    "        y_true=all_targets.argmax(axis=1),\n",
    "        preds=test_preds.argmax(axis=1),\n",
    "        class_names=labels\n",
    "    )})\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# Sweep configuration\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': 'mimic-cxr-sweep',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        'batch_size': {'values': [16, 32, 64]},\n",
    "        'epochs': {'values': [15, 30, 50]},\n",
    "        'learning_rate': {'max': 0.01, 'min': 0.0001, 'distribution': 'log_uniform'},\n",
    "        'weight_decay': {'max': 0.1, 'min': 1e-5, 'distribution': 'log_uniform'},\n",
    "        'optimizer': {'values': ['adam', 'sgd', 'rmsprop', 'adamw']},\n",
    "        'architecture': {'values': ['densenet121', 'resnet50', 'efficientnet', 'inceptionv3', 'vgg16', \n",
    "                                    'densenet201', 'resnext101', 'efficientnet_b4', 'vit']},\n",
    "        'lr_scheduler': {'values': ['step', 'cosine']},\n",
    "        'lr_step_size': {'values': [5, 10, 15]},\n",
    "        'lr_gamma': {'min': 0.1, 'max': 0.5, 'distribution': 'uniform'}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='name')\n",
    "\n",
    "# Run the sweep\n",
    "wandb.agent(sweep_id, function=train_model, count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c431d-58b9-4b8c-b002-e5a4719a6835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
