{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac8f9d8-efb5-4771-8588-99c96a5e2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# RAG\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOllama\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from adjustText import adjust_text\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import densenet121\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a15275-44a8-4ed8-bf70-3e13cba3f47f",
   "metadata": {},
   "source": [
    "## Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01974a5d-3cd2-4177-b690-384a1f90d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels as used during training\n",
    "labels = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', \n",
    "          'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion', 'Lung Opacity', \n",
    "          'No Finding', 'Pleural Effusion', 'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices']\n",
    "\n",
    "class DenseNet121(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.densenet121 = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Identity()  # No classifier yet, only features\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.densenet121(x)\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.resnet50.fc.in_features\n",
    "        self.resnet50.fc = nn.Identity()  # No classifier yet, only features\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet50(x)\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, out_size=14):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        # Instantiate DenseNet121 and ResNet50 for frontal and lateral views\n",
    "        self.densenet_frontal = DenseNet121()\n",
    "        self.resnet_frontal = ResNet50()\n",
    "        \n",
    "        self.densenet_lateral = DenseNet121()\n",
    "        self.resnet_lateral = ResNet50()\n",
    "        \n",
    "        # The combined feature size\n",
    "        frontal_feature_size = 1024 + 2048  # Assuming DenseNet121 outputs 1024 and ResNet50 outputs 2048\n",
    "        lateral_feature_size = 1024 + 2048\n",
    "        \n",
    "        combined_feature_size = frontal_feature_size + lateral_feature_size\n",
    "        \n",
    "        # Final classifier layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_feature_size, out_size),\n",
    "            nn.Sigmoid()  # Assuming binary classification for multi-label\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_frontal, x_lateral):\n",
    "        # Extract features from DenseNet and ResNet for both views\n",
    "        frontal_features = torch.cat([self.densenet_frontal(x_frontal), self.resnet_frontal(x_frontal)], dim=1)\n",
    "        lateral_features = torch.cat([self.densenet_lateral(x_lateral), self.resnet_lateral(x_lateral)], dim=1)\n",
    "        \n",
    "        # Combine frontal and lateral features\n",
    "        combined_features = torch.cat([frontal_features, lateral_features], dim=1)\n",
    "        \n",
    "        # Final output through classifier\n",
    "        out = self.classifier(combined_features)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CombinedModel().to(device)\n",
    "model.load_state_dict(torch.load('path_to_directory'))\n",
    "model.eval()\n",
    "\n",
    "# CLAHE transform class\n",
    "class CLAHETransform:\n",
    "    def __init__(self, clip_limit=0.10, tile_grid_size=(8, 8)):\n",
    "        self.clip_limit = clip_limit\n",
    "        self.tile_grid_size = tile_grid_size\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = np.array(img)\n",
    "        if img.ndim == 3:\n",
    "            lab_img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "            l_channel, a_channel, b_channel = cv2.split(lab_img)\n",
    "            l_channel = self.clahe.apply(l_channel)\n",
    "            lab_img = cv2.merge((l_channel, a_channel, b_channel))\n",
    "            img = cv2.cvtColor(lab_img, cv2.COLOR_LAB2RGB)\n",
    "        else:\n",
    "            img = self.clahe.apply(img)\n",
    "        return Image.fromarray(img.astype('uint8'))\n",
    "\n",
    "# Define the validation transform (same as during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    CLAHETransform(clip_limit=0.35, tile_grid_size=(8, 8)),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Function to predict an image\n",
    "def predict_image(frontal_image_path, lateral_image_path):\n",
    "    # Load and preprocess the frontal image\n",
    "    frontal_image = Image.open(frontal_image_path).convert('RGB')\n",
    "    frontal_image = transform(frontal_image)\n",
    "    frontal_image = frontal_image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    # Load and preprocess the lateral image\n",
    "    lateral_image = Image.open(lateral_image_path).convert('RGB')\n",
    "    lateral_image = transform(lateral_image)\n",
    "    lateral_image = lateral_image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(frontal_image, lateral_image)\n",
    "    \n",
    "    # Convert predictions to numpy array and map to labels\n",
    "    predictions = output.cpu().numpy().squeeze()  # Remove batch dimension\n",
    "    pred_scores = {labels[i]: predictions[i] for i in range(len(predictions))}\n",
    "    \n",
    "    return pred_scores\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# frontal_image_path = 'text_reports/testing/IMAGES/p13952691_s54551451.jpg'\n",
    "# lateral_image_path = 'text_reports/testing/IMAGES/p13952691_s54551451.jpg'\n",
    "# pred_scores = predict_image(frontal_image_path, lateral_image_path)\n",
    "\n",
    "# pred_scores\n",
    "# Print the scores for each label\n",
    "# for label, score in pred_scores.items():\n",
    "#     print(f'{label}: {score:.4f}')\n",
    "\n",
    "def get_top_classifications(classifications: dict, top_n: int = 3, threshold: float = 0.50) -> dict:\n",
    "    # Filter classifications that are above the threshold\n",
    "    filtered_classifications = {k: v for k, v in classifications.items() if v > threshold}\n",
    "    \n",
    "    # Sort the filtered classifications by their score in descending order\n",
    "    sorted_classifications = sorted(filtered_classifications.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    # Return the top N classifications\n",
    "    top_classifications = {k: v for k, v in sorted_classifications[:top_n]}\n",
    "    \n",
    "    return top_classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f456fcb-d04a-4d3e-895c-be041701cbd1",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf56fd5-418b-40ae-8688-a4dbcd7e06c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the embedding model and vector database\n",
    "oembed = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "vectordb = Chroma(\n",
    "    persist_directory=\"text_reports/all_embed_db\",\n",
    "    embedding_function=oembed\n",
    ")\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "ollama_llm = ChatOllama(base_url=\"http://localhost:11434\", model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690596e1-2102-4320-89fb-2a6764f564b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_report = \"\"\"\n",
    "EXAMINATION: CHEST (PA AND LAT)\n",
    "\n",
    "INDICATION: ___F with new onset ascites\n",
    "\n",
    "TECHNIQUE: Chest PA and lateral\n",
    "\n",
    "COMPARISON: None.\n",
    "\n",
    "FINDINGS: \n",
    "There is no focal consolidation, pleural effusion or pneumothorax. Bilateral nodular opacities that most likely represent nipple shadows. The cardiomediastinal silhouette is normal. Clips project over the left lung, potentially within the breast. The imaged upper abdomen is unremarkable. Chronic deformity of the posterior left sixth and seventh ribs are noted.\n",
    "\n",
    "IMPRESSION: \n",
    "No acute cardiopulmonary process.\n",
    "\"\"\"\n",
    "\n",
    "# Define the report template\n",
    "report_template = \"\"\"\n",
    "1. EXAMINATION: {examination}\n",
    "2. INDICATION: {indication}\n",
    "3. TECHNIQUE: {technique}\n",
    "4. COMPARISON: {comparison}\n",
    "5. FINDINGS: {findings}\n",
    "6. IMPRESSION: {impression}\n",
    "\"\"\"\n",
    "\n",
    "# Set up the prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are an expert radiologist. Generate a concise and accurate radiology report based on the provided context and classification. Follow these strict guidelines:\n",
    "    \n",
    "    1. Use ONLY the given template structure. Do not add any sections or text outside this structure.\n",
    "    2. Keep EXAMINATION, INDICATION, TECHNIQUE, and COMPARISON brief, as link sample report.\n",
    "    3. FINDINGS should be detailed but focused, describing only relevant observations.\n",
    "    4. IMPRESSION should summarize key findings and their clinical significance concisely.\n",
    "    5. Do not include any introductory or concluding statements, notifications, or recommendations unless explicitly part of the findings or impression.\n",
    "    6. Use appropriate medical terminology and maintain a professional tone.\n",
    "    7. Base your report solely on the given context and classification.\n",
    "    \n",
    "    Classification: {classifications}\n",
    "    Context: {context}\n",
    "    Query: {query}\n",
    "\n",
    "    Generate the report now, strictly following the template below:\n",
    "    {report_template}\n",
    "\n",
    "    Here's a sample report for reference:\n",
    "    {sample_report}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c8438f-103a-496f-be18-daedf241476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLMChain with the prompt template and Ollama LLM\n",
    "llm_chain = LLMChain(llm=ollama_llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969ef9a-2718-43fb-82db-92a565f00eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to embed the query\n",
    "def embed_query_and_caption(query, image_caption):\n",
    "    query_embedding = oembed.embed_query(query)\n",
    "    caption_embedding = oembed.embed_query(image_caption)\n",
    "    # Combine embeddings (you can experiment with different combination methods)\n",
    "    combined_embedding = (np.array(query_embedding) + np.array(caption_embedding)) / 2\n",
    "    return combined_embedding.tolist()\n",
    "    \n",
    "# Function to retrieve similar documents\n",
    "def retrieve_similar_documents(combined_embedding, k=5):\n",
    "    return vectordb.similarity_search_by_vector(combined_embedding, k=k)\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def calculate_cosine_similarity(vec1, vec2):\n",
    "    # Compute the cosine similarity between two vectors\n",
    "    dot_product = np.dot(vec1, vec2.T)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb100702-4426-4c93-a10b-e664eefcfff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the generate_medical_report function\n",
    "def generate_medical_report(classifications, image_caption):\n",
    "    # Construct a more detailed query\n",
    "\n",
    "    query = (\n",
    "        f\"Examine the chest X-ray for the presence of {', '.join(classifications)}. \"\n",
    "        \"Provide a comprehensive analysis of the observed radiographic features and their clinical implications.\"\n",
    "    )\n",
    "    \n",
    "    # Combine query and image caption embeddings\n",
    "    combined_embedding = embed_query_and_caption(query, image_caption)\n",
    "    \n",
    "    # Retrieve similar documents using the combined embedding\n",
    "    similar_docs = retrieve_similar_documents(combined_embedding)\n",
    "    \n",
    "    # Extract the content from similar documents\n",
    "    context = \"\\n\".join([doc.page_content for doc in similar_docs])\n",
    "    \n",
    "    # Show the retrieved documents\n",
    "    print(\"Retrieved Documents:\")\n",
    "    for i, doc in enumerate(similar_docs, 1):\n",
    "        print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "    \n",
    "    # Run the LLM chain with context, query, classifications, and image caption\n",
    "    result = llm_chain.run({\n",
    "        \"context\": context,\n",
    "        \"query\": query,\n",
    "        \"classifications\": \", \".join(classifications),\n",
    "        \"sample_report\": sample_report,\n",
    "        \"image_caption\": image_caption,\n",
    "        \"report_template\": report_template\n",
    "    })\n",
    "    \n",
    "    # Parse the output for clean presentation\n",
    "    parsed_result = StrOutputParser().parse(result)\n",
    "    \n",
    "    print(\"Generated Medical Report:\")\n",
    "    print(parsed_result)\n",
    "\n",
    "    return parsed_result\n",
    "    \n",
    "    # Visualize the query, image caption, and similarity of the retrieved documents\n",
    "    # visualize_embeddings(query, image_caption, similar_docs, oembed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed05b422-ead1-4397-8890-57a2ede8ae7c",
   "metadata": {},
   "source": [
    "## Llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18986496-945c-4405-8bf4-1322cc1dfd6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_directory(frontal_image_dir, lateral_image_dir, caption_csv, report_csv, report_dir):\n",
    "    # Load the captions from maira-2-frontal-results.csv\n",
    "    captions = {}\n",
    "    with open(caption_csv, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            captions[row['frontal_image']] = row['final_output']  # Image name as key, final_output as value\n",
    "\n",
    "    # Prepare the output CSV\n",
    "\n",
    "    with open(report_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "       \n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Image', 'Top Classifications', 'Generated Report', 'Original Report'])\n",
    "\n",
    "        # Process each image in the directory\n",
    "        for image_name in os.listdir(frontal_image_dir):\n",
    "           \n",
    "            if image_name in captions:  # Ensure the image has a caption\n",
    "                print(\"herhe\")\n",
    "                frontal_image_path = os.path.join(frontal_image_dir, image_name)\n",
    "                lateral_image_path = os.path.join(lateral_image_dir, image_name)\n",
    "                \n",
    "                image_caption = captions[image_name]\n",
    "\n",
    "                # Get predictions and classifications\n",
    "                pred_score = predict_image(frontal_image_path, lateral_image_path)\n",
    "      \n",
    "                for label, score in pred_score.items():\n",
    "                    print(f'{label}: {score:.4f}')\n",
    "                    \n",
    "                classifications = get_top_classifications(pred_score)\n",
    "\n",
    "                print(f\"\\nTop classification: {classifications}. Image Caption: {image_caption}\\n\")\n",
    "                \n",
    "                # Generate a medical report\n",
    "                generated_report = generate_medical_report(classifications, image_caption)\n",
    "\n",
    "\n",
    "                # Retrieve the original report\n",
    "                report_filename = image_name.rsplit('.', 1)[0] + '.txt'\n",
    "                original_report_path = os.path.join(report_dir, report_filename)\n",
    "                original_report = ''\n",
    "                try:\n",
    "                    with open(original_report_path, 'r') as file:\n",
    "                        original_report = file.read()\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"Warning: Original report not found for {report_filename}\")\n",
    "                \n",
    "                # Save results to the CSV\n",
    "                writer.writerow([\n",
    "                    image_name,\n",
    "                    ', '.join(classifications.keys()),\n",
    "                    generated_report,\n",
    "                    original_report\n",
    "                ])\n",
    "\n",
    "                print(f\"\\nProcessed: {image_name}\")\n",
    "\n",
    "    print(f\"Processing complete. Results saved to {report_csv}\")\n",
    "\n",
    "# Paths\n",
    "frontal_image_dir = 'path_to_directory'\n",
    "lateral_image_dir = 'path_to_directory'\n",
    "caption_csv = 'path_to_directory'\n",
    "report_dir = 'path_to_directory'\n",
    "report_csv = 'path_to_directory'\n",
    "\n",
    "# Process directory\n",
    "process_directory(frontal_image_dir, lateral_image_dir, caption_csv, report_csv, report_dir)\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# classifications = [\"Atelectasis\", \"Cardiomegaly\", \"Pneumonia\"]\n",
    "\n",
    "# image_caption = \"The patient is status post median sternotomy and cabg. The heart is moderately enlarged. The aorta is tortuous. There is mild pulmonary vascular congestion. Small bilateral pleural effusions are noted. Streaky opacities in the lung bases likely reflect atelectasis. No pneumothorax is identified. There are no acute osseous abnormalities.\"\n",
    "# generate_medical_report(classifications, image_caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
