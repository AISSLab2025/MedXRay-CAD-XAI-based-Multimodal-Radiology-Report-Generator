{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f7b694-084c-478e-bff7-272799c72271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import densenet121\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a37afc1-ffcb-4dbe-b718-9b7a051e1986",
   "metadata": {},
   "source": [
    "## Classification Best model Inferencace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1572390e-b02e-4a3b-9858-cec319086305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels as used during training\n",
    "labels = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', \n",
    "          'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion', 'Lung Opacity', \n",
    "          'No Finding', 'Pleural Effusion', 'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices']\n",
    "\n",
    "class DenseNet121(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.densenet121 = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Identity()  # No classifier yet, only features\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.densenet121(x)\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.resnet50.fc.in_features\n",
    "        self.resnet50.fc = nn.Identity()  # No classifier yet, only features\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet50(x)\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, out_size=14):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        # Instantiate DenseNet121 and ResNet50 for frontal and lateral views\n",
    "        self.densenet_frontal = DenseNet121()\n",
    "        self.resnet_frontal = ResNet50()\n",
    "        \n",
    "        self.densenet_lateral = DenseNet121()\n",
    "        self.resnet_lateral = ResNet50()\n",
    "        \n",
    "        # The combined feature size\n",
    "        frontal_feature_size = 1024 + 2048  # Assuming DenseNet121 outputs 1024 and ResNet50 outputs 2048\n",
    "        lateral_feature_size = 1024 + 2048\n",
    "        \n",
    "        combined_feature_size = frontal_feature_size + lateral_feature_size\n",
    "        \n",
    "        # Final classifier layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_feature_size, out_size),\n",
    "            nn.Sigmoid()  # Assuming binary classification for multi-label\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_frontal, x_lateral):\n",
    "        # Extract features from DenseNet and ResNet for both views\n",
    "        frontal_features = torch.cat([self.densenet_frontal(x_frontal), self.resnet_frontal(x_frontal)], dim=1)\n",
    "        lateral_features = torch.cat([self.densenet_lateral(x_lateral), self.resnet_lateral(x_lateral)], dim=1)\n",
    "        \n",
    "        # Combine frontal and lateral features\n",
    "        combined_features = torch.cat([frontal_features, lateral_features], dim=1)\n",
    "        \n",
    "        # Final output through classifier\n",
    "        out = self.classifier(combined_features)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CombinedModel().to(device)\n",
    "model.load_state_dict(torch.load('path_to_directory'))\n",
    "model.eval()\n",
    "\n",
    "# CLAHE transform class\n",
    "class CLAHETransform:\n",
    "    def __init__(self, clip_limit=0.10, tile_grid_size=(8, 8)):\n",
    "        self.clip_limit = clip_limit\n",
    "        self.tile_grid_size = tile_grid_size\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = np.array(img)\n",
    "        if img.ndim == 3:\n",
    "            lab_img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "            l_channel, a_channel, b_channel = cv2.split(lab_img)\n",
    "            l_channel = self.clahe.apply(l_channel)\n",
    "            lab_img = cv2.merge((l_channel, a_channel, b_channel))\n",
    "            img = cv2.cvtColor(lab_img, cv2.COLOR_LAB2RGB)\n",
    "        else:\n",
    "            img = self.clahe.apply(img)\n",
    "        return Image.fromarray(img.astype('uint8'))\n",
    "\n",
    "# Define the validation transform (same as during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    CLAHETransform(clip_limit=0.35, tile_grid_size=(8, 8)),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Function to predict an image\n",
    "def predict_image(frontal_image_path, lateral_image_path):\n",
    "    # Load and preprocess the frontal image\n",
    "    frontal_image = Image.open(frontal_image_path).convert('RGB')\n",
    "    frontal_image = transform(frontal_image)\n",
    "    frontal_image = frontal_image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    # Load and preprocess the lateral image\n",
    "    lateral_image = Image.open(lateral_image_path).convert('RGB')\n",
    "    lateral_image = transform(lateral_image)\n",
    "    lateral_image = lateral_image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(frontal_image, lateral_image)\n",
    "    \n",
    "    # Convert predictions to numpy array and map to labels\n",
    "    predictions = output.cpu().numpy().squeeze()  # Remove batch dimension\n",
    "    pred_scores = {labels[i]: predictions[i] for i in range(len(predictions))}\n",
    "    \n",
    "    return pred_scores\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# frontal_image_path = 'text_reports/testing/IMAGES/p13952691_s54551451.jpg'\n",
    "# lateral_image_path = 'text_reports/testing/IMAGES/p13952691_s54551451.jpg'\n",
    "# pred_scores = predict_image(frontal_image_path, lateral_image_path)\n",
    "\n",
    "# pred_scores\n",
    "# Print the scores for each label\n",
    "# for label, score in pred_scores.items():\n",
    "#     print(f'{label}: {score:.4f}')\n",
    "\n",
    "def get_top_classifications(classifications: dict, top_n: int = 3, threshold: float = 0.50) -> dict:\n",
    "    # Filter classifications that are above the threshold\n",
    "    filtered_classifications = {k: v for k, v in classifications.items() if v > threshold}\n",
    "    \n",
    "    # Sort the filtered classifications by their score in descending order\n",
    "    sorted_classifications = sorted(filtered_classifications.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    # Return the top N classifications\n",
    "    top_classifications = {k: v for k, v in sorted_classifications[:top_n]}\n",
    "    \n",
    "    return top_classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7e6715-9414-4b2a-83f1-3bac6b351e93",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0924e565-d509-472c-b485-5299fc120287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the embedding model and vector database\n",
    "oembed = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "vectordb = Chroma(\n",
    "    persist_directory=\"text_reports/all_embed_db\",\n",
    "    embedding_function=oembed\n",
    ")\n",
    "\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb164b-b796-43c7-b5d3-cc6005d9c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Ollama LLM\n",
    "ollama_llm = ChatOllama(base_url=\"http://localhost:11434\", model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e96aa-dcec-4622-b99b-6c3eccb1583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_report = \"\"\"\n",
    "EXAMINATION: CHEST (PA AND LAT)\n",
    "\n",
    "INDICATION: ___ with new onset ascites\n",
    "\n",
    "TECHNIQUE: Chest PA and lateral\n",
    "\n",
    "COMPARISON: None.\n",
    "\n",
    "FINDINGS: \n",
    "There is no focal consolidation, pleural effusion or pneumothorax. Bilateral nodular opacities that most likely represent nipple shadows. The cardiomediastinal silhouette is normal. Clips project over the left lung, potentially within the breast. The imaged upper abdomen is unremarkable. Chronic deformity of the posterior left sixth and seventh ribs are noted.\n",
    "\n",
    "IMPRESSION: \n",
    "No acute cardiopulmonary process.\n",
    "\"\"\"\n",
    "\n",
    "report_template = \"\"\"\n",
    "1. EXAMINATION: {examination}\n",
    "2. INDICATION: {indication}\n",
    "3. TECHNIQUE: {technique}\n",
    "4. COMPARISON: {comparison}\n",
    "5. FINDINGS: {findings}\n",
    "6. IMPRESSION: {impression}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"classification\", \"context\", \"report_template\", \"sample_report\"],\n",
    "    template=\"\"\"\n",
    "    You are an expert radiologist. Generate a concise and accurate radiology report based on the provided context and classification. Follow these strict guidelines:\n",
    "\n",
    "    1. Use ONLY the given template structure. Do not add any sections or text outside this structure.\n",
    "    2. Keep EXAMINATION, INDICATION, TECHNIQUE, and COMPARISON brief, as link sample report.\n",
    "    3. FINDINGS should be detailed but focused, describing only relevant observations.\n",
    "    4. IMPRESSION should summarize key findings and their clinical significance concisely.\n",
    "    5. Do not include any introductory or concluding statements, notifications, or recommendations unless explicitly part of the findings or impression.\n",
    "    6. Use appropriate medical terminology and maintain a professional tone.\n",
    "    7. Base your report solely on the given context and classification.\n",
    "    \n",
    "    Classification: {classification}\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Generate the report now, strictly following the template below:\n",
    "    {report_template}\n",
    "\n",
    "    Here's a sample report for reference:\n",
    "    {sample_report}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6819a120-9d7e-47a1-b9b9-95c14a879676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# embeddings_filter = EmbeddingsFilter(embeddings=oembed, similarity_threshold=0.60)\n",
    "# compression_retriever = ContextualCompressionRetriever(\n",
    "#             base_compressor=embeddings_filter, base_retriever=retriever\n",
    "#         )\n",
    "        \n",
    "# query = f\"\"\"Retrieve relevant information for a chest radiograph report focusing on Support Devices, Cardiomegaly. \n",
    "#     Include details about:\n",
    "#     1. Radiological appearances of Support Devices, Cardiomegaly\n",
    "#     2. Associated findings and complications\n",
    "#     3. Typical locations and distributions\n",
    "#     4. Differentiation between Support Devices, Cardiomegaly\n",
    "#     5. Severity indicators and extent assessment\n",
    "#     6. Any other relevant chest radiograph findings that might co-occur\n",
    "    \n",
    "#     Provide specific medical terminology and descriptions used in radiology reports for these conditions.\"\"\"\n",
    "    \n",
    "# compressed_docs = compression_retriever.invoke(query)\n",
    "# retrieved_docs = pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451d0ddb-3da2-47bb-8d53-93a30e8cf421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_compression_retrieval(vectordb, llm, classifications):\n",
    "    # Extract only classification names\n",
    "    classification_names = \", \".join(classifications.keys())\n",
    "    \n",
    "    print(f\"\\nClassifications name: {classification_names}\")\n",
    "\n",
    "    embeddings_filter = EmbeddingsFilter(embeddings=oembed, similarity_threshold=0.50)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=embeddings_filter, base_retriever=retriever\n",
    "        )\n",
    "        \n",
    "    query = f\"\"\"Retrieve relevant information for a chest radiograph report focusing on {classification_names}. \n",
    "    Include details about:\n",
    "    1. Radiological appearances of {classification_names}\n",
    "    2. Associated findings and complications\n",
    "    3. Typical locations and distributions\n",
    "    4. Differentiation between {classification_names}\n",
    "    5. Severity indicators and extent assessment\n",
    "    6. Any other relevant chest radiograph findings that might co-occur\n",
    "    \n",
    "    Provide specific medical terminology and descriptions used in radiology reports for these conditions.\"\"\"\n",
    "    \n",
    "    compressed_docs = compression_retriever.invoke(query)\n",
    "    retrieved_docs = pretty_print_docs(compressed_docs)\n",
    "\n",
    "    # Combine the retrieved documents into a single context string\n",
    "    context = \"\\n\".join([doc.page_content for doc in compressed_docs])\n",
    "\n",
    "    print(f\"context: {context}\")\n",
    "    # Create an LLMChain\n",
    "    llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    \n",
    "    # Generate the report\n",
    "    result = llm_chain.run(\n",
    "        classification=classification_names,\n",
    "        context=context,\n",
    "        report_template=report_template,\n",
    "        sample_report=sample_report\n",
    "    )\n",
    "\n",
    "    print(f\"\\n Final Report \\n {result}\")\n",
    "    return result, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05883d06-f455-48cc-badf-e2272296ca93",
   "metadata": {},
   "source": [
    "## llama3-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dac838-d2e2-4684-a40c-7fabfff734ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def process_image_directory(frontal_image_dir, lateral_image_dir, report_dir, vectordb, llm, output_csv_path):\n",
    "    with open(output_csv_path, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['image_name', 'Classifications', 'generated_report', 'original_report'])\n",
    "\n",
    "        # Process each image\n",
    "        for image_filename in os.listdir(frontal_image_dir):\n",
    "            # Ensure the file is an image\n",
    "            if not image_filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                continue\n",
    "\n",
    "            frontal_image_path = os.path.join(frontal_image_dir, image_filename)\n",
    "            lateral_image_path = os.path.join(lateral_image_dir, image_filename)\n",
    "            \n",
    "            # Predict the classifications\n",
    "            pred_scores = predict_image(frontal_image_path, lateral_image_path)\n",
    "            for label, score in pred_scores.items():\n",
    "                print(f'{label}: {score:.4f}')\n",
    "                \n",
    "            classifications = get_top_classifications(pred_scores)\n",
    "\n",
    "            # Retrieve the original report\n",
    "            report_filename = image_filename.rsplit('.', 1)[0] + '.txt'\n",
    "            original_report_path = os.path.join(report_dir, report_filename)\n",
    "            original_report = ''\n",
    "            try:\n",
    "                with open(original_report_path, 'r') as file:\n",
    "                    original_report = file.read()\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: Original report not found for {report_filename}\")\n",
    "\n",
    "            # Generate the RAG-based report\n",
    "            generated_report, _ = rag_compression_retrieval(vectordb, llm, classifications)\n",
    "\n",
    "            # Save the results to the CSV file\n",
    "            writer.writerow([\n",
    "                image_filename,\n",
    "                ', '.join(classifications.keys()),\n",
    "                generated_report,\n",
    "                original_report\n",
    "            ])\n",
    "\n",
    "            print(f\"Processed: {image_filename}\")\n",
    "\n",
    "# Example usage\n",
    "frontal_image_dir = 'path_to_directory'\n",
    "lateral_image_dir = 'path_to_directory'\n",
    "report_dir = 'path_to_directory'\n",
    "output_csv_path = 'path_to_directory'\n",
    "\n",
    "process_image_directory(frontal_image_dir, lateral_image_dir, report_dir, vectordb, ollama_llm, output_csv_path)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# image_path = r'D:\\CODES\\classification_and_llm\\testing\\frontal_classification\\images\\p10773491_s54418703.jpg'\n",
    "# pred_scores = predict_image(image_path)\n",
    "# # pred_scores\n",
    "# # Print the scores for each label\n",
    "# for label, score in pred_scores.items():\n",
    "#     print(f'{label}: {score:.4f}')\n",
    "\n",
    "# classifications = get_top_classifications(pred_scores)\n",
    "# print(f\"\\nTop classifications: {classifications}\")\n",
    "\n",
    "# final_report, retrieved_docs = rag_compression_retrieval(vectordb, ollama_llm, classifications)\n",
    "\n",
    "# print(\"Final Report:\")\n",
    "# print(final_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
